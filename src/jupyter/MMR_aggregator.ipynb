{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.core.frame import DataFrame #class\n",
    "from configuration.configuration import Configuration #class\n",
    "from typing import List #class\n",
    "from typing import Set #class\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from datasets.aDataset import ADataset #class\n",
    "from datasets.datasetML import DatasetML #class\n",
    "from datasets.datasetRetailrocket import DatasetRetailRocket #class\n",
    "from datasets.datasetST import DatasetST #class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ToolMMR():\n",
    "\n",
    "    def init(self, trainDataset:ADataset):\n",
    "        if  not isinstance(trainDataset, ADataset):\n",
    "            raise ValueError(\"Argument trainDataset isn't type ADataset.\")\n",
    "\n",
    "        if type(trainDataset) is DatasetML:\n",
    "            cbDataPath = Configuration.cbML1MDataFileWithPathOHE\n",
    "        elif type(trainDataset) is DatasetRetailRocket:\n",
    "            return None\n",
    "\n",
    "        elif type(trainDataset) is DatasetST:\n",
    "            cbDataPath = Configuration.cbSTDataFileWithPathOHE\n",
    "\n",
    "        mmrCBFeatures = pd.read_csv(cbDataPath, sep=\",\", header=0, index_col=0)\n",
    "        # print(mmrCBFeatures)\n",
    "\n",
    "        dfCBSim = 1 - pairwise_distances(mmrCBFeatures, metric=\"cosine\")\n",
    "        self.mmrCBSim: DataFrame = DataFrame(data=dfCBSim, index=mmrCBFeatures.index, columns=mmrCBFeatures.index)\n",
    "        # print(self.CBSim.shape)\n",
    "\n",
    "    def _argmax(self, keys, f):\n",
    "        return max(keys, key=f)\n",
    "\n",
    "    def _objects_similarity(self, i:int, j:int):\n",
    "        # print(i,j, self.mmrCBSim.at[i, j])\n",
    "        try:\n",
    "            return self.mmrCBSim.at[i, j]\n",
    "        except:\n",
    "            #print(\"miss\")\n",
    "            return 0\n",
    "    \n",
    "    def mmr_sorted_with_prefix(self, lambda_:float, results, prefix, length:int):\n",
    "        selected = pd.Series(dtype=np.float64)\n",
    "        prefix = set(prefix)\n",
    "        docs = set(results.index)\n",
    "        while (len(selected) < len(docs)) and (len(selected) < length):\n",
    "            remaining = docs - set(selected.index)\n",
    "            mmr_score = lambda x: lambda_ * results[x] - (1 - lambda_) * max(\n",
    "                [self._objects_similarity(x, y) for y in set(selected.index).union(prefix) - {x}] or [\n",
    "                    0])  # TODO: self.mmr_objects_similarity\n",
    "            next_selected = self._argmax(remaining, mmr_score)\n",
    "            mmrVal = lambda_ + (lambda_ * results[next_selected] - (1 - lambda_) * max(\n",
    "                [self._objects_similarity(next_selected, y) for y in set(selected.index).union(prefix) - {next_selected}] or [0]))\n",
    "            selected = selected.append(pd.Series(mmrVal, index=[next_selected]))\n",
    "            # selected[next_selected] = mmrVal\n",
    "\n",
    "        return selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.aDataset import ADataset #class\n",
    "from datasets.datasetST import DatasetST #class\n",
    "\n",
    "from datasets.ml.items import Items #class\n",
    "from datasets.slantour.serials import Serials #class\n",
    "from datasets.slantour.events import Events #class\n",
    "\n",
    "dataset:ADataset = DatasetST.readDatasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolMMR = ToolMMR()\n",
    "toolMMR.init(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ToolMMR at 0x1af15572f48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolMMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pandas.core.frame import DataFrame #class\n",
    "\n",
    "from aggregation.aggrFuzzyDHondt import AggrFuzzyDHondt #class\n",
    "from aggregation.aggrFuzzyDHondtINF import AggrFuzzyDHondtINF #class\n",
    "\n",
    "import pandas as pd\n",
    "from history.aHistory import AHistory #class\n",
    "from history.historyDF import HistoryDF #class\n",
    "\n",
    "from userBehaviourDescription.userBehaviourDescription import UserBehaviourDescription #class\n",
    "from userBehaviourDescription.userBehaviourDescription import observationalLinearProbabilityFnc #function\n",
    "\n",
    "#from aggregation.toolsDHontNF.penalizationOfResultsByNegImpFeedback.penalUsingReduceRelevance import PenalizationOfResultsByNegImpFeedbackUsingReduceRelevance #class\n",
    "from aggregation.negImplFeedback.penalUsingReduceRelevance import PenalUsingReduceRelevance #class\n",
    "\n",
    "from aggregation.negImplFeedback.penalUsingReduceRelevance import penaltyLinear #function\n",
    "\n",
    "from aggregation.negImplFeedback.penalUsingReduceRelevance import PenalUsingReduceRelevance #class\n",
    "from aggregation.negImplFeedback.aPenalization import APenalization #class\n",
    "from aggregation.aggrWeightedAVG import AggrWeightedAVG #class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130    0.258333\n",
      "5516    0.225000\n",
      "4232    0.158333\n",
      "7522    0.091667\n",
      "2685    0.091667\n",
      "64      0.075000\n",
      "5422    0.066667\n",
      "7511    0.033333\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "\n",
    "# method results, items=[1,2,4,5,6,7,8,12,32,64,77]\n",
    "methodsResultDict = {\n",
    "          \"metoda1\":pd.Series([0.2,0.1,0.3,0.3,0.1],[6130,5422,5516,4232,2685],name=\"rating\"),\n",
    "          \"metoda2\":pd.Series([0.1,0.1,0.2,0.3,0.3],[4232,7511,7522,5516,6130],name=\"rating\"),\n",
    "          \"metoda3\":pd.Series([0.3,0.1,0.2,0.3,0.1],[6130,5422,2685,64,7522],name=\"rating\")\n",
    "          }\n",
    "# methods parametes\n",
    "portfolioModelData:List[tuple] = [['metoda1',100], ['metoda2',80], ['metoda3',60]]\n",
    "portfolioModel:DataFrame = pd.DataFrame(portfolioModelData, columns=[\"methodID\",\"votes\"])\n",
    "portfolioModel.set_index(\"methodID\", inplace=True)\n",
    "\n",
    "sumMethodsVotes: float = portfolioModel[\"votes\"].sum()\n",
    "for methodIdI in portfolioModel.index:\n",
    "        portfolioModel.loc[methodIdI, \"votes\"] = portfolioModel.loc[methodIdI, \"votes\"] / sumMethodsVotes\n",
    "\n",
    "\n",
    "aggr:AggrWeightedAVG = AggrWeightedAVG(HistoryDF(\"\"), {})\n",
    "#itemIDs:int = aggr.run(methodsResultDict, methodsParamsDF, N)\n",
    "#print(itemIDs)\n",
    "\n",
    "userID:int = 0\n",
    "itemID:int = 7\n",
    "\n",
    "itemIDs = aggr.runWithScore(methodsResultDict, portfolioModel, userID, N*5, {})\n",
    "print(itemIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64      0.537500\n",
       "6130    0.337699\n",
       "7511    0.285408\n",
       "2685    0.268989\n",
       "7522    0.206284\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previousRecs = [6130,5516,4232,7522,2685]\n",
    "itemIDs:List[tuple] = aggr.runWithScore(methodsResultDict, portfolioModel, userID, N*5, {})\n",
    "results = toolMMR.mmr_sorted_with_prefix(0.5, itemIDs, previousRecs, 5)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 6130, 7511, 2685, 7522]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.index.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
